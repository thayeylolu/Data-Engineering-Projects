{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd0d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990b1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sagemaker-containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d21a3",
   "metadata": {},
   "source": [
    "### importing libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb5957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import sagemaker as sg\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import geopy.distance\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f7d7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula import api\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38af63a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'collections' has no attribute 'Mapping'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpkl\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker_containers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entry_point\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker_xgboost_container\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dmatrix\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker_xgboost_container\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributed\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker_containers/entry_point.py:23\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List  \u001b[38;5;66;03m# noqa ignore=F401 imported but unused\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mretrying\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m retry\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker_containers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _entry_point_type, _env, _files, _modules, _runner\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[1;32m     27\u001b[0m     uri,\n\u001b[1;32m     28\u001b[0m     user_entry_point,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m ):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# type: (str, str, List[str], Dict[str, str], bool, bool, _runner.RunnerType,Dict[str, str]) -> None  # pylint: disable=line-too-long # noqa ignore=E501\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download, prepare and executes a compressed tar file from S3 or provided directory as an user\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    entrypoint. Runs the user entry point, passing env_vars as environment variables and args\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    as command arguments.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m            executing the entry point.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker_containers/_env.py:29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker_containers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _content_types, _logging, _mapping, _params\n\u001b[1;32m     31\u001b[0m logger \u001b[38;5;241m=\u001b[39m _logging\u001b[38;5;241m.\u001b[39mget_logger()\n\u001b[1;32m     33\u001b[0m SAGEMAKER_BASE_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: str\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker_containers/_mapping.py:151\u001b[0m\n\u001b[1;32m    146\u001b[0m     excluded_items \u001b[38;5;241m=\u001b[39m {k: dictionary[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dictionary\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m included_items}\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SplitResultSpec(included\u001b[38;5;241m=\u001b[39mincluded_items, excluded\u001b[38;5;241m=\u001b[39mexcluded_items)\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMappingMixin\u001b[39;00m(\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMapping\u001b[49m):\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Placeholder docstring\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproperties\u001b[39m(\u001b[38;5;28mself\u001b[39m):  \u001b[38;5;66;03m# type: () -> list\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'collections' has no attribute 'Mapping'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import pickle as pkl\n",
    "\n",
    "from sagemaker_containers import entry_point\n",
    "from sagemaker_xgboost_container.data_utils import get_dmatrix\n",
    "from sagemaker_xgboost_container import distributed\n",
    "\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10,6]\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# aws role and region\n",
    "role = sg.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "print(region)\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814883b1",
   "metadata": {},
   "source": [
    "### Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c48dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from s3\n",
    "bucket_og = \"ml-data-sceince-bucket\"\n",
    "prefix_og_file = \"uberfare\"\n",
    "\n",
    "bucket = 'sagemaker-us-west-2-920733537674'\n",
    "filename = \"uber.csv\"\n",
    "\n",
    "# S3 URL\n",
    "data_s3_location = \"s3://{}/{}/{}\".format(bucket_og, prefix_og_file, filename)  \n",
    "df = pd.read_csv(data_s3_location)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088bd3a",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21611c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnamed column \n",
    "df.drop(['Unnamed: 0','key'], axis=1, inplace=True)\n",
    "display(df.head())\n",
    "\n",
    "target = 'fare_amount'\n",
    "features = [i for i in df.columns if i not in [target]]\n",
    "\n",
    "print('\\n Descriptive: The Datset consists of {} features & {} samples.'.format(df.shape[1], df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3dcad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find null\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59fc397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reframing the columns\n",
    "\n",
    "df = df[(df.pickup_latitude<90) & (df.dropoff_latitude<90) &\n",
    "        (df.pickup_latitude>-90) & (df.dropoff_latitude>-90) &\n",
    "        (df.pickup_longitude<180) & (df.dropoff_longitude<180) &\n",
    "        (df.pickup_longitude>-180) & (df.dropoff_longitude>-180)]\n",
    "\n",
    "df.pickup_datetime=pd.to_datetime(df.pickup_datetime)\n",
    "\n",
    "df['year'] = df.pickup_datetime.dt.year\n",
    "df['month'] = df.pickup_datetime.dt.month\n",
    "df['weekday'] = df.pickup_datetime.dt.weekday\n",
    "df['hour'] = df.pickup_datetime.dt.hour\n",
    "\n",
    "df['Distance']=[round(geopy.distance.distance((df.pickup_latitude[i], df.pickup_longitude[i]),(df.dropoff_latitude[i], df.dropoff_longitude[i])).m,2) for i in df.index]\n",
    "\n",
    "df.drop(['pickup_datetime','month'], axis=1, inplace=True)\n",
    "\n",
    "original_df = df.copy(deep=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the stats of all the columns\n",
    "\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns =df.columns[df.columns.str.contains('latitude|longitude', case = False)])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c655a6e",
   "metadata": {},
   "source": [
    "### Data Upload and Write Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return (\n",
    "        boto3.Session(region_name=region)\n",
    "        .resource(\"s3\")\n",
    "        .Bucket(bucket)\n",
    "        .Object(key)\n",
    "        .upload_fileobj(fobj)\n",
    "    )\n",
    "\n",
    "\n",
    "def upload_to_s3(bucket, channel, filename):\n",
    "    fobj = open(filename, \"rb\")\n",
    "    prefix = \"data/uberfare\"\n",
    "    key = prefix + \"/\" + channel\n",
    "    \n",
    "\n",
    "    url = \"s3://{}/{}/{}\".format(bucket, key, filename)\n",
    "    print(\"Writing to {}\".format(url))\n",
    "    write_to_s3(fobj, bucket, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sg.Session()\n",
    "file_name = \"cleaned_dataset.csv\"\n",
    "df.to_csv(file_name)\n",
    "session.upload_data(path= file_name, bucket=bucket, key_prefix= \"data/uberfare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac86fbf",
   "metadata": {},
   "source": [
    "### Data Splititng "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac398084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_split(\n",
    "    FILE_DATA,\n",
    "    DATA_DIR,\n",
    "    FILE_TRAIN_BASE,\n",
    "    FILE_TRAIN_1,\n",
    "    FILE_VALIDATION,\n",
    "    FILE_TEST,\n",
    "    PERCENT_TRAIN_0,\n",
    "    PERCENT_TRAIN_1,\n",
    "    PERCENT_VALIDATION,\n",
    "    PERCENT_TEST,\n",
    "):\n",
    "    data = [l for l in open(FILE_DATA, \"r\")]\n",
    "    train_file_0 = open(DATA_DIR + \"/\" + FILE_TRAIN_0, \"w\")\n",
    "    train_file_1 = open(DATA_DIR + \"/\" + FILE_TRAIN_1, \"w\")\n",
    "    valid_file = open(DATA_DIR + \"/\" + FILE_VALIDATION, \"w\")\n",
    "    tests_file = open(DATA_DIR + \"/\" + FILE_TEST, \"w\")\n",
    "\n",
    "    num_of_data = len(data)\n",
    "    num_train_0 = int((PERCENT_TRAIN_0 / 100.0) * num_of_data)\n",
    "    num_train_1 = int((PERCENT_TRAIN_1 / 100.0) * num_of_data)\n",
    "    num_valid = int((PERCENT_VALIDATION / 100.0) * num_of_data)\n",
    "    num_tests = int((PERCENT_TEST / 100.0) * num_of_data)\n",
    "\n",
    "    data_fractions = [num_train_0, num_train_1, num_valid, num_tests]\n",
    "    split_data = [[], [], [], []]\n",
    "\n",
    "    rand_data_ind = 0\n",
    "\n",
    "    for split_ind, fraction in enumerate(data_fractions):\n",
    "        for i in range(fraction):\n",
    "            rand_data_ind = random.randint(0, len(data) - 1)\n",
    "            split_data[split_ind].append(data[rand_data_ind])\n",
    "            data.pop(rand_data_ind)\n",
    "\n",
    "    for l in split_data[0]:\n",
    "        train_file_0.write(l)\n",
    "\n",
    "    for l in split_data[1]:\n",
    "        train_file_1.write(l)\n",
    "\n",
    "    for l in split_data[2]:\n",
    "        valid_file.write(l)\n",
    "\n",
    "    for l in split_data[3]:\n",
    "        tests_file.write(l)\n",
    "\n",
    "    train_file_0.close()\n",
    "    train_file_1.close()\n",
    "    valid_file.close()\n",
    "    tests_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251812eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "FILE_DATA = \"cleaned_dataset.csv\"\n",
    "\n",
    "\n",
    "# Split the downloaded data into train/test/validation files\n",
    "FILE_TRAIN_0 = \"train_0\"\n",
    "FILE_TRAIN_1 = \"train_1\"\n",
    "FILE_VALIDATION = \"validation\"\n",
    "FILE_TEST = \"test\"\n",
    "PERCENT_TRAIN_0 = 35\n",
    "PERCENT_TRAIN_1 = 35\n",
    "PERCENT_VALIDATION = 15\n",
    "PERCENT_TEST = 15\n",
    "\n",
    "DATA_DIR = \"data/uberfare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2761218",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "data_split(\n",
    "    FILE_DATA,\n",
    "    DATA_DIR,\n",
    "    FILE_TRAIN_0,\n",
    "    FILE_TRAIN_1,\n",
    "    FILE_VALIDATION,\n",
    "    FILE_TEST,\n",
    "    PERCENT_TRAIN_0,\n",
    "    PERCENT_TRAIN_1,\n",
    "    PERCENT_VALIDATION,\n",
    "    PERCENT_TEST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the files to the S3 bucket\n",
    "upload_to_s3(bucket, \"train/train_0.csv\", DATA_DIR + \"/\" + FILE_TRAIN_0)\n",
    "upload_to_s3(bucket, \"train/train_1.csv\", DATA_DIR + \"/\" + FILE_TRAIN_1)\n",
    "upload_to_s3(bucket, \"validation/validation.csv\", DATA_DIR + \"/\" + FILE_VALIDATION)\n",
    "upload_to_s3(bucket, \"test/test.csv\", DATA_DIR + \"/\" + FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a150272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the files to the S3 bucket\n",
    "upload_to_s3(bucket, \"train_0.csv\")\n",
    "upload_to_s3(bucket, \"train_1.csv\" )\n",
    "upload_to_s3(bucket, \"validation.csv\" )\n",
    "upload_to_s3(bucket, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04432d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
